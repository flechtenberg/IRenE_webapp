{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to IRenE Webapp Documentation","text":"<p>IRenE (Information Retrieval and Extraction) is a Flask web application designed to assist researchers in retrieving and exploring academic literature using the Scopus API.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Upload Seed Documents: Upload PDFs to extract initial keywords.</li> <li>Refine Keywords: Adjust and weight keywords for more precise sampling.</li> <li>Iterative Sampling: Perform iterative sampling using the Scopus API.</li> <li>View Results: Browse and download a ranked list of relevant papers.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To begin using the IRenE Webapp, follow these steps:</p> <ol> <li>Upload Seed Corpus: Provide your seed PDF documents.</li> <li>Upload Scopus API Key: Authenticate with the Scopus API.</li> <li>Extract Keywords: Generate initial keywords from your documents.</li> <li>Refine Keywords: Adjust the keywords based on your research needs.</li> <li>Start Sampling: Begin the iterative sampling process to retrieve relevant papers.</li> </ol>"},{"location":"#api-documentation","title":"API Documentation","text":"<p>Detailed API documentation is available below.</p> <ul> <li>Processing Module</li> <li>App Module</li> </ul>"},{"location":"app/","title":"App Module","text":""},{"location":"app/#backend.processing.PDFProcessingError","title":"<code>PDFProcessingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for PDF processing errors.</p> Source code in <code>backend\\processing.py</code> <pre><code>class PDFProcessingError(Exception):\n    \"\"\"Custom exception for PDF processing errors.\"\"\"\n    pass\n</code></pre>"},{"location":"app/#backend.processing.construct_search_query","title":"<code>construct_search_query(selected_keywords)</code>","text":"<p>Construct a search query string using logical AND.</p> Source code in <code>backend\\processing.py</code> <pre><code>def construct_search_query(selected_keywords):\n    \"\"\"\n    Construct a search query string using logical AND.\n    \"\"\"\n    return ' AND '.join(selected_keywords)\n</code></pre>"},{"location":"app/#backend.processing.execute_search_scopus","title":"<code>execute_search_scopus(query, scopus_api_key, threshold=1000)</code>","text":"<p>Execute the search query using the Scopus API via elsapy.</p> <p>Parameters: - query: Search query string. - scopus_api_key: Dict containing 'apikey' and 'insttoken'.</p> <p>Returns: - match_count: Number of matching articles. - matched_papers: Set of unique paper identifiers (e.g., Links).</p> Source code in <code>backend\\processing.py</code> <pre><code>def execute_search_scopus(query, scopus_api_key, threshold=1000):\n    \"\"\"\n    Execute the search query using the Scopus API via elsapy.\n\n    Parameters:\n    - query: Search query string.\n    - scopus_api_key: Dict containing 'apikey' and 'insttoken'.\n\n    Returns:\n    - match_count: Number of matching articles.\n    - matched_papers: Set of unique paper identifiers (e.g., Links).\n    \"\"\"\n    headers = {\n        'X-ELS-APIKey': scopus_api_key['apikey'],\n        'X-ELS-Insttoken': scopus_api_key['insttoken']\n    }\n\n    client = ElsClient(scopus_api_key['apikey'])\n    client.inst_token = scopus_api_key['insttoken']\n\n    search_query = f'TITLE-ABS-KEY({query})'\n    doc_srch = ElsSearch(search_query, 'scopus')\n\n    try:\n        doc_srch.execute(client, get_all=False)\n        num_results = doc_srch.tot_num_res\n\n        if num_results &gt; 0 and num_results &lt;= threshold:\n            doc_srch.execute(client, get_all=True)\n            data = doc_srch.results\n            matched_papers = []\n            for entry in data:\n                paper_info = {\n                    'scopus_id': entry.get('dc:identifier', '').replace('SCOPUS_ID:', ''),\n                    'first_author': entry.get('dc:creator', '-'),\n                    'year': entry.get('prism:coverDate', '-')[:4],\n                    'title': entry.get('dc:title', '-'),\n                    'journal': entry.get('prism:publicationName', '-'),\n                    'citations': entry.get('citedby-count', '0'),\n                    'open_access': entry.get('openaccess', '-'),\n                    'link': entry.get('link', [{}])[2].get('@href', '#')  # Usually the third link is the scopus link\n                }\n                matched_papers.append(paper_info)\n            match_count = len(matched_papers)\n            return match_count, matched_papers\n        else:\n            return num_results, set()\n\n    except RequestException as req_err:\n        # Handle issues with network or API request\n        logger.error(f\"Network or API request error during Scopus API call: {req_err}\")\n        return 0, set()\n\n    except KeyError as key_err:\n        # Handle missing data in the response\n        logger.error(f\"Missing expected data in Scopus API response: {key_err}\")\n        return 0, set()\n\n    except Exception as e:\n        # General catch-all for other unforeseen errors\n        logger.error(f\"Unexpected error during Scopus API call: {e}\")\n        return 0, set()\n</code></pre>"},{"location":"app/#backend.processing.extract_seed","title":"<code>extract_seed(files)</code>","text":"<p>Extract text from a list of uploaded PDF files and return a list of preprocessed strings.</p> <p>Each element in the returned list corresponds to the text content of one PDF document. The text is preprocessed (e.g., cleaned of stopwords and punctuation) for further use.</p> <p>Parameters: - files (list): A list of PDF files uploaded by the user.</p> <p>Returns: - list: A list of preprocessed text strings, where each string represents the content of one PDF file.</p> Source code in <code>backend\\processing.py</code> <pre><code>def extract_seed(files):\n    \"\"\"\n    Extract text from a list of uploaded PDF files and return a list of preprocessed strings.\n\n    Each element in the returned list corresponds to the text content of one PDF document.\n    The text is preprocessed (e.g., cleaned of stopwords and punctuation) for further use.\n\n    Parameters:\n    - files (list): A list of PDF files uploaded by the user.\n\n    Returns:\n    - list: A list of preprocessed text strings, where each string represents the content of one PDF file.\n    \"\"\"\n    seed_texts = []\n    for file in files:\n        text = extract_text_from_pdf(file)\n        seed_texts.append(text)\n    return seed_texts\n</code></pre>"},{"location":"app/#backend.processing.extract_text_from_pdf","title":"<code>extract_text_from_pdf(file)</code>","text":"<p>Extract and preprocess text from a single PDF file.</p> <p>This function reads through all the pages of the provided PDF file, extracts the text, and performs preprocessing (such as cleaning, lowercasing, and tokenization) on the extracted text for further use in natural language processing tasks.</p> <p>Parameters: - file: A PDF file object from which text needs to be extracted.</p> <p>Returns: - str: A single preprocessed string containing the combined text from all pages of the PDF, ready for further analysis.</p> Source code in <code>backend\\processing.py</code> <pre><code>def extract_text_from_pdf(file):\n    \"\"\"\n    Extract and preprocess text from a single PDF file.\n\n    This function reads through all the pages of the provided PDF file, extracts the text, and performs preprocessing\n    (such as cleaning, lowercasing, and tokenization) on the extracted text for further use in natural language processing tasks.\n\n    Parameters:\n    - file: A PDF file object from which text needs to be extracted.\n\n    Returns:\n    - str: A single preprocessed string containing the combined text from all pages of the PDF, ready for further analysis.\n    \"\"\"\n    try:\n        reader = PyPDF2.PdfReader(file)\n        text = \"\"\n        for page in reader.pages:\n            extracted_text = page.extract_text()\n            if extracted_text:\n                text += extracted_text + \" \"\n\n        # Preprocess the extracted text\n        text = preprocess_text(text)\n        return text\n\n    except PdfReadError as e:\n        # Raise a custom error and stop processing\n        raise PDFProcessingError(f\"Error processing PDF file '{file}': {e}\")\n    except Exception as e:\n        # Raise a custom error for any other general issue\n        raise PDFProcessingError(f\"An unexpected error occurred while processing '{file}': {e}\")\n</code></pre>"},{"location":"app/#backend.processing.get_keywords","title":"<code>get_keywords(seed_texts, num_keywords)</code>","text":"<p>Extract the top 'num_keywords' keywords from a list of documents using TF-IDF (Term Frequency-Inverse Document Frequency).</p> <p>This function performs the following steps: - Combines custom stop words with standard English stop words. - Initializes a TfidfVectorizer to convert the text data into a matrix of TF-IDF features, considering unigrams only. - Fits the TF-IDF model to the provided documents (seed_texts). - Sums the TF-IDF scores across all documents to rank the importance of each keyword. - Filters out numbers and stop words from the resulting keywords.</p> <p>Parameters: - seed_texts (list of str): A list where each element represents the (filtered) text of a document. - num_keywords (int): The number of top keywords to extract.</p> <ul> <li>list of dict: A list of dictionaries containing the top keywords and their corresponding TF-IDF scores.   Each dictionary has two keys:<ul> <li>'word': The keyword.</li> <li>'weight': The TF-IDF score, rounded to two decimal places.</li> </ul> </li> </ul> Source code in <code>backend\\processing.py</code> <pre><code>def get_keywords(seed_texts, num_keywords):\n    \"\"\"\n    Extract the top 'num_keywords' keywords from a list of documents using TF-IDF (Term Frequency-Inverse Document Frequency).\n\n    This function performs the following steps:\n    - Combines custom stop words with standard English stop words.\n    - Initializes a TfidfVectorizer to convert the text data into a matrix of TF-IDF features, considering unigrams only.\n    - Fits the TF-IDF model to the provided documents (seed_texts).\n    - Sums the TF-IDF scores across all documents to rank the importance of each keyword.\n    - Filters out numbers and stop words from the resulting keywords.\n\n    Parameters:\n    - seed_texts (list of str): A list where each element represents the (filtered) text of a document.\n    - num_keywords (int): The number of top keywords to extract.\n\n    Returns:\n    - list of dict: A list of dictionaries containing the top keywords and their corresponding TF-IDF scores.\n      Each dictionary has two keys:\n        - 'word': The keyword.\n        - 'weight': The TF-IDF score, rounded to two decimal places.\n    \"\"\"\n\n    # Combine with English stop words from TfidfVectorizer\n    combined_stop_words = list(set(ENGLISH_STOP_WORDS).union(additional_stop_words))\n\n    # Initialize TfidfVectorizer with extended stop words and improved tokenization\n    vectorizer = TfidfVectorizer(\n        stop_words=combined_stop_words,  # Now a list\n        max_features=num_keywords,\n        token_pattern=r'\\b[a-zA-Z]{2,}\\b',  # Tokens with at least two letters\n        ngram_range=(1, 1),  # Include unigrams and bigrams\n        smooth_idf=True,\n        sublinear_tf=True\n    )\n\n    # Fit and transform the list of documents\n    tfidf_matrix = vectorizer.fit_transform(seed_texts)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Sum TF-IDF scores across all documents\n    scores = tfidf_matrix.sum(axis=0).A1  # Convert to 1D array\n    keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n\n    # Filter and select top keywords\n    filtered_keywords = []\n    for word, weight in keywords:\n        words = word.split()\n        if any(w in combined_stop_words for w in words):\n            continue\n        if re.search(r'\\d', word):\n            continue\n        filtered_keywords.append({'word': word, 'weight': round(weight, 2)})\n        if len(filtered_keywords) == num_keywords:\n            break\n\n    return filtered_keywords\n</code></pre>"},{"location":"app/#backend.processing.preprocess_text","title":"<code>preprocess_text(text)</code>","text":"<p>Preprocess the input text by applying various cleaning and filtering steps for NLP tasks.</p> <p>This function performs the following operations: - Converts the text to lowercase. - Removes URLs and numeric values. - Utilizes spaCy for lemmatization, and filters out stop words, punctuation, and non-alphabetic tokens. - Excludes tokens related to specific named entities (organizations, people, geopolitical entities, dates) and certain parts of speech (proper nouns, numbers). - Custom stop words are also removed.</p> <p>Parameters: - text (str): The raw text to be processed.</p> <p>Returns: - str: A cleaned and preprocessed string where tokens have been lemmatized and unnecessary elements have been removed.</p> Source code in <code>backend\\processing.py</code> <pre><code>def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text by applying various cleaning and filtering steps for NLP tasks.\n\n    This function performs the following operations:\n    - Converts the text to lowercase.\n    - Removes URLs and numeric values.\n    - Utilizes spaCy for lemmatization, and filters out stop words, punctuation, and non-alphabetic tokens.\n    - Excludes tokens related to specific named entities (organizations, people, geopolitical entities, dates) and certain parts of speech (proper nouns, numbers).\n    - Custom stop words are also removed.\n\n    Parameters:\n    - text (str): The raw text to be processed.\n\n    Returns:\n    - str: A cleaned and preprocessed string where tokens have been lemmatized and unnecessary elements have been removed.\n    \"\"\"\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove numbers\n    text = re.sub(r'\\b\\d+\\b', '', text)\n\n\n    # Use spaCy to process text and remove stop words, punctuation, etc.\n    doc = nlp(text)\n    cleaned_tokens = []\n    for token in doc:\n        if not token.is_stop and not token.is_punct and token.is_alpha:\n            # Exclude certain named entities or tokens\n            if token.text in additional_stop_words:\n                continue\n            if token.ent_type_ in {'ORG', 'PERSON', 'GPE', 'DATE'}:\n                continue\n            if token.pos_ in {'PROPN', 'NUM'}:\n                continue\n            cleaned_tokens.append(token.lemma_)\n\n    cleaned_text = ' '.join(cleaned_tokens)\n    return cleaned_text\n</code></pre>"},{"location":"app/#backend.processing.scopus_sampling_process","title":"<code>scopus_sampling_process(weight_dict, threshold, outer_iterations=5, progress_callback=None, scopus_api_key=None)</code>","text":"<p>Perform the sampling process using Scopus API with outer and inner iterations.</p> <p>Parameters: - weight_dict: Dict of keywords and their weights. - threshold: The match count threshold. - outer_iterations: Number of separate sampling runs. - progress_callback: Function to call with progress updates. - scopus_api_key: Dict containing 'apikey' and 'insttoken'.</p> <p>Returns: - ranked_papers: List of dictionaries containing paper information, sorted by occurrences.</p> Source code in <code>backend\\processing.py</code> <pre><code>def scopus_sampling_process(weight_dict, threshold, outer_iterations=5, progress_callback=None, scopus_api_key=None):\n    \"\"\"\n    Perform the sampling process using Scopus API with outer and inner iterations.\n\n    Parameters:\n    - weight_dict: Dict of keywords and their weights.\n    - threshold: The match count threshold.\n    - outer_iterations: Number of separate sampling runs.\n    - progress_callback: Function to call with progress updates.\n    - scopus_api_key: Dict containing 'apikey' and 'insttoken'.\n\n    Returns:\n    - ranked_papers: List of dictionaries containing paper information, sorted by occurrences.\n    \"\"\"\n    if not scopus_api_key:\n        logger.warning(\"No Scopus API Key provided. Cannot perform real sampling.\")\n        return []\n\n    keywords = list(weight_dict.keys())\n    weights = list(weight_dict.values())\n\n    paper_rank_counts = {}\n\n    for outer in range(1, outer_iterations + 1):\n        print(f\"\\n--- Outer Iteration {outer} ---\")\n        search_keywords = []\n        while True:\n            selected_keyword = weighted_random_selection(keywords, weights)\n            if not selected_keyword:\n                logger.warning(\"No keyword selected. Ending inner iterations.\")\n                break\n            # Prevent adding duplicate keywords\n            if selected_keyword in search_keywords:\n                print(f\"Keyword '{selected_keyword}' already in query. Selecting a different keyword.\")\n                continue\n            search_keywords.append(selected_keyword)\n            query = construct_search_query(search_keywords)\n            match_count, matched_papers = execute_search_scopus(query, scopus_api_key, threshold)\n            print(f\"Added '{selected_keyword}' | Query: '{query}' | Matches: {match_count}\")\n\n            # Update progress\n            if progress_callback:\n                progress_callback(outer, query, match_count)\n\n            if match_count &lt; threshold:\n                print(f\"Match count {match_count} below threshold {threshold}. Ending inner iterations.\")\n                break\n\n            # Add a small delay to simulate processing time\n            time.sleep(0.1)\n\n        # Record matched papers from the final inner iteration\n        for paper in matched_papers:\n            scopus_id = paper['scopus_id']\n            if scopus_id in paper_rank_counts:\n                paper_rank_counts[scopus_id]['occurrences'] += 1\n            else:\n                paper['occurrences'] = 1\n                paper_rank_counts[scopus_id] = paper\n\n            #print(f\"Recorded paper: {scopus_id} | Current occurrences: {paper_rank_counts[scopus_id]['occurrences']}\")\n\n        # Add a small delay after each outer iteration\n        time.sleep(0.1)\n\n    # Create a ranked list sorted by occurrences descending\n    ranked_papers = sorted(paper_rank_counts.values(), key=lambda x: x['occurrences'], reverse=True)\n    print(\"\\n--- Sampling Completed ---\")\n    return ranked_papers\n</code></pre>"},{"location":"app/#backend.processing.weighted_random_selection","title":"<code>weighted_random_selection(keywords, weights)</code>","text":"<p>Select a keyword based on weights.</p> <p>Parameters: - keywords: List of keywords. - weights: Corresponding list of weights.</p> <p>Returns: - Selected keyword or None if no selection possible.</p> Source code in <code>backend\\processing.py</code> <pre><code>def weighted_random_selection(keywords, weights):\n    \"\"\"\n    Select a keyword based on weights.\n\n    Parameters:\n    - keywords: List of keywords.\n    - weights: Corresponding list of weights.\n\n    Returns:\n    - Selected keyword or None if no selection possible.\n    \"\"\"\n    total_weight = sum(weights)\n    if total_weight == 0:\n        logger.info(\"Total weight is zero. No keyword can be selected.\")\n        return None\n    probabilities = [w / total_weight for w in weights]\n    selected_keyword = np.random.choice(keywords, p=probabilities)\n\n    try:\n        selected_index = keywords.index(selected_keyword)\n        weight = weights[selected_index]\n        print(f\"Selected keyword: '{selected_keyword}' with weight {weight}\", flush=True)\n    except ValueError:\n        logger.error(f\"Selected keyword '{selected_keyword}' not found in keywords list.\", flush=True)\n\n    return selected_keyword\n</code></pre>"},{"location":"processing/","title":"Processing Module","text":""},{"location":"processing/#backend.processing.PDFProcessingError","title":"<code>PDFProcessingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for PDF processing errors.</p> Source code in <code>backend\\processing.py</code> <pre><code>class PDFProcessingError(Exception):\n    \"\"\"Custom exception for PDF processing errors.\"\"\"\n    pass\n</code></pre>"},{"location":"processing/#backend.processing.construct_search_query","title":"<code>construct_search_query(selected_keywords)</code>","text":"<p>Construct a search query string using logical AND.</p> Source code in <code>backend\\processing.py</code> <pre><code>def construct_search_query(selected_keywords):\n    \"\"\"\n    Construct a search query string using logical AND.\n    \"\"\"\n    return ' AND '.join(selected_keywords)\n</code></pre>"},{"location":"processing/#backend.processing.execute_search_scopus","title":"<code>execute_search_scopus(query, scopus_api_key, threshold=1000)</code>","text":"<p>Execute the search query using the Scopus API via elsapy.</p> <p>Parameters: - query: Search query string. - scopus_api_key: Dict containing 'apikey' and 'insttoken'.</p> <p>Returns: - match_count: Number of matching articles. - matched_papers: Set of unique paper identifiers (e.g., Links).</p> Source code in <code>backend\\processing.py</code> <pre><code>def execute_search_scopus(query, scopus_api_key, threshold=1000):\n    \"\"\"\n    Execute the search query using the Scopus API via elsapy.\n\n    Parameters:\n    - query: Search query string.\n    - scopus_api_key: Dict containing 'apikey' and 'insttoken'.\n\n    Returns:\n    - match_count: Number of matching articles.\n    - matched_papers: Set of unique paper identifiers (e.g., Links).\n    \"\"\"\n    headers = {\n        'X-ELS-APIKey': scopus_api_key['apikey'],\n        'X-ELS-Insttoken': scopus_api_key['insttoken']\n    }\n\n    client = ElsClient(scopus_api_key['apikey'])\n    client.inst_token = scopus_api_key['insttoken']\n\n    search_query = f'TITLE-ABS-KEY({query})'\n    doc_srch = ElsSearch(search_query, 'scopus')\n\n    try:\n        doc_srch.execute(client, get_all=False)\n        num_results = doc_srch.tot_num_res\n\n        if num_results &gt; 0 and num_results &lt;= threshold:\n            doc_srch.execute(client, get_all=True)\n            data = doc_srch.results\n            matched_papers = []\n            for entry in data:\n                paper_info = {\n                    'scopus_id': entry.get('dc:identifier', '').replace('SCOPUS_ID:', ''),\n                    'first_author': entry.get('dc:creator', '-'),\n                    'year': entry.get('prism:coverDate', '-')[:4],\n                    'title': entry.get('dc:title', '-'),\n                    'journal': entry.get('prism:publicationName', '-'),\n                    'citations': entry.get('citedby-count', '0'),\n                    'open_access': entry.get('openaccess', '-'),\n                    'link': entry.get('link', [{}])[2].get('@href', '#')  # Usually the third link is the scopus link\n                }\n                matched_papers.append(paper_info)\n            match_count = len(matched_papers)\n            return match_count, matched_papers\n        else:\n            return num_results, set()\n\n    except RequestException as req_err:\n        # Handle issues with network or API request\n        logger.error(f\"Network or API request error during Scopus API call: {req_err}\")\n        return 0, set()\n\n    except KeyError as key_err:\n        # Handle missing data in the response\n        logger.error(f\"Missing expected data in Scopus API response: {key_err}\")\n        return 0, set()\n\n    except Exception as e:\n        # General catch-all for other unforeseen errors\n        logger.error(f\"Unexpected error during Scopus API call: {e}\")\n        return 0, set()\n</code></pre>"},{"location":"processing/#backend.processing.extract_seed","title":"<code>extract_seed(files)</code>","text":"<p>Extract text from a list of uploaded PDF files and return a list of preprocessed strings.</p> <p>Each element in the returned list corresponds to the text content of one PDF document. The text is preprocessed (e.g., cleaned of stopwords and punctuation) for further use.</p> <p>Parameters: - files (list): A list of PDF files uploaded by the user.</p> <p>Returns: - list: A list of preprocessed text strings, where each string represents the content of one PDF file.</p> Source code in <code>backend\\processing.py</code> <pre><code>def extract_seed(files):\n    \"\"\"\n    Extract text from a list of uploaded PDF files and return a list of preprocessed strings.\n\n    Each element in the returned list corresponds to the text content of one PDF document.\n    The text is preprocessed (e.g., cleaned of stopwords and punctuation) for further use.\n\n    Parameters:\n    - files (list): A list of PDF files uploaded by the user.\n\n    Returns:\n    - list: A list of preprocessed text strings, where each string represents the content of one PDF file.\n    \"\"\"\n    seed_texts = []\n    for file in files:\n        text = extract_text_from_pdf(file)\n        seed_texts.append(text)\n    return seed_texts\n</code></pre>"},{"location":"processing/#backend.processing.extract_text_from_pdf","title":"<code>extract_text_from_pdf(file)</code>","text":"<p>Extract and preprocess text from a single PDF file.</p> <p>This function reads through all the pages of the provided PDF file, extracts the text, and performs preprocessing (such as cleaning, lowercasing, and tokenization) on the extracted text for further use in natural language processing tasks.</p> <p>Parameters: - file: A PDF file object from which text needs to be extracted.</p> <p>Returns: - str: A single preprocessed string containing the combined text from all pages of the PDF, ready for further analysis.</p> Source code in <code>backend\\processing.py</code> <pre><code>def extract_text_from_pdf(file):\n    \"\"\"\n    Extract and preprocess text from a single PDF file.\n\n    This function reads through all the pages of the provided PDF file, extracts the text, and performs preprocessing\n    (such as cleaning, lowercasing, and tokenization) on the extracted text for further use in natural language processing tasks.\n\n    Parameters:\n    - file: A PDF file object from which text needs to be extracted.\n\n    Returns:\n    - str: A single preprocessed string containing the combined text from all pages of the PDF, ready for further analysis.\n    \"\"\"\n    try:\n        reader = PyPDF2.PdfReader(file)\n        text = \"\"\n        for page in reader.pages:\n            extracted_text = page.extract_text()\n            if extracted_text:\n                text += extracted_text + \" \"\n\n        # Preprocess the extracted text\n        text = preprocess_text(text)\n        return text\n\n    except PdfReadError as e:\n        # Raise a custom error and stop processing\n        raise PDFProcessingError(f\"Error processing PDF file '{file}': {e}\")\n    except Exception as e:\n        # Raise a custom error for any other general issue\n        raise PDFProcessingError(f\"An unexpected error occurred while processing '{file}': {e}\")\n</code></pre>"},{"location":"processing/#backend.processing.get_keywords","title":"<code>get_keywords(seed_texts, num_keywords)</code>","text":"<p>Extract the top 'num_keywords' keywords from a list of documents using TF-IDF (Term Frequency-Inverse Document Frequency).</p> <p>This function performs the following steps: - Combines custom stop words with standard English stop words. - Initializes a TfidfVectorizer to convert the text data into a matrix of TF-IDF features, considering unigrams only. - Fits the TF-IDF model to the provided documents (seed_texts). - Sums the TF-IDF scores across all documents to rank the importance of each keyword. - Filters out numbers and stop words from the resulting keywords.</p> <p>Parameters: - seed_texts (list of str): A list where each element represents the (filtered) text of a document. - num_keywords (int): The number of top keywords to extract.</p> <ul> <li>list of dict: A list of dictionaries containing the top keywords and their corresponding TF-IDF scores.   Each dictionary has two keys:<ul> <li>'word': The keyword.</li> <li>'weight': The TF-IDF score, rounded to two decimal places.</li> </ul> </li> </ul> Source code in <code>backend\\processing.py</code> <pre><code>def get_keywords(seed_texts, num_keywords):\n    \"\"\"\n    Extract the top 'num_keywords' keywords from a list of documents using TF-IDF (Term Frequency-Inverse Document Frequency).\n\n    This function performs the following steps:\n    - Combines custom stop words with standard English stop words.\n    - Initializes a TfidfVectorizer to convert the text data into a matrix of TF-IDF features, considering unigrams only.\n    - Fits the TF-IDF model to the provided documents (seed_texts).\n    - Sums the TF-IDF scores across all documents to rank the importance of each keyword.\n    - Filters out numbers and stop words from the resulting keywords.\n\n    Parameters:\n    - seed_texts (list of str): A list where each element represents the (filtered) text of a document.\n    - num_keywords (int): The number of top keywords to extract.\n\n    Returns:\n    - list of dict: A list of dictionaries containing the top keywords and their corresponding TF-IDF scores.\n      Each dictionary has two keys:\n        - 'word': The keyword.\n        - 'weight': The TF-IDF score, rounded to two decimal places.\n    \"\"\"\n\n    # Combine with English stop words from TfidfVectorizer\n    combined_stop_words = list(set(ENGLISH_STOP_WORDS).union(additional_stop_words))\n\n    # Initialize TfidfVectorizer with extended stop words and improved tokenization\n    vectorizer = TfidfVectorizer(\n        stop_words=combined_stop_words,  # Now a list\n        max_features=num_keywords,\n        token_pattern=r'\\b[a-zA-Z]{2,}\\b',  # Tokens with at least two letters\n        ngram_range=(1, 1),  # Include unigrams and bigrams\n        smooth_idf=True,\n        sublinear_tf=True\n    )\n\n    # Fit and transform the list of documents\n    tfidf_matrix = vectorizer.fit_transform(seed_texts)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Sum TF-IDF scores across all documents\n    scores = tfidf_matrix.sum(axis=0).A1  # Convert to 1D array\n    keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n\n    # Filter and select top keywords\n    filtered_keywords = []\n    for word, weight in keywords:\n        words = word.split()\n        if any(w in combined_stop_words for w in words):\n            continue\n        if re.search(r'\\d', word):\n            continue\n        filtered_keywords.append({'word': word, 'weight': round(weight, 2)})\n        if len(filtered_keywords) == num_keywords:\n            break\n\n    return filtered_keywords\n</code></pre>"},{"location":"processing/#backend.processing.preprocess_text","title":"<code>preprocess_text(text)</code>","text":"<p>Preprocess the input text by applying various cleaning and filtering steps for NLP tasks.</p> <p>This function performs the following operations: - Converts the text to lowercase. - Removes URLs and numeric values. - Utilizes spaCy for lemmatization, and filters out stop words, punctuation, and non-alphabetic tokens. - Excludes tokens related to specific named entities (organizations, people, geopolitical entities, dates) and certain parts of speech (proper nouns, numbers). - Custom stop words are also removed.</p> <p>Parameters: - text (str): The raw text to be processed.</p> <p>Returns: - str: A cleaned and preprocessed string where tokens have been lemmatized and unnecessary elements have been removed.</p> Source code in <code>backend\\processing.py</code> <pre><code>def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text by applying various cleaning and filtering steps for NLP tasks.\n\n    This function performs the following operations:\n    - Converts the text to lowercase.\n    - Removes URLs and numeric values.\n    - Utilizes spaCy for lemmatization, and filters out stop words, punctuation, and non-alphabetic tokens.\n    - Excludes tokens related to specific named entities (organizations, people, geopolitical entities, dates) and certain parts of speech (proper nouns, numbers).\n    - Custom stop words are also removed.\n\n    Parameters:\n    - text (str): The raw text to be processed.\n\n    Returns:\n    - str: A cleaned and preprocessed string where tokens have been lemmatized and unnecessary elements have been removed.\n    \"\"\"\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove numbers\n    text = re.sub(r'\\b\\d+\\b', '', text)\n\n\n    # Use spaCy to process text and remove stop words, punctuation, etc.\n    doc = nlp(text)\n    cleaned_tokens = []\n    for token in doc:\n        if not token.is_stop and not token.is_punct and token.is_alpha:\n            # Exclude certain named entities or tokens\n            if token.text in additional_stop_words:\n                continue\n            if token.ent_type_ in {'ORG', 'PERSON', 'GPE', 'DATE'}:\n                continue\n            if token.pos_ in {'PROPN', 'NUM'}:\n                continue\n            cleaned_tokens.append(token.lemma_)\n\n    cleaned_text = ' '.join(cleaned_tokens)\n    return cleaned_text\n</code></pre>"},{"location":"processing/#backend.processing.scopus_sampling_process","title":"<code>scopus_sampling_process(weight_dict, threshold, outer_iterations=5, progress_callback=None, scopus_api_key=None)</code>","text":"<p>Perform the sampling process using Scopus API with outer and inner iterations.</p> <p>Parameters: - weight_dict: Dict of keywords and their weights. - threshold: The match count threshold. - outer_iterations: Number of separate sampling runs. - progress_callback: Function to call with progress updates. - scopus_api_key: Dict containing 'apikey' and 'insttoken'.</p> <p>Returns: - ranked_papers: List of dictionaries containing paper information, sorted by occurrences.</p> Source code in <code>backend\\processing.py</code> <pre><code>def scopus_sampling_process(weight_dict, threshold, outer_iterations=5, progress_callback=None, scopus_api_key=None):\n    \"\"\"\n    Perform the sampling process using Scopus API with outer and inner iterations.\n\n    Parameters:\n    - weight_dict: Dict of keywords and their weights.\n    - threshold: The match count threshold.\n    - outer_iterations: Number of separate sampling runs.\n    - progress_callback: Function to call with progress updates.\n    - scopus_api_key: Dict containing 'apikey' and 'insttoken'.\n\n    Returns:\n    - ranked_papers: List of dictionaries containing paper information, sorted by occurrences.\n    \"\"\"\n    if not scopus_api_key:\n        logger.warning(\"No Scopus API Key provided. Cannot perform real sampling.\")\n        return []\n\n    keywords = list(weight_dict.keys())\n    weights = list(weight_dict.values())\n\n    paper_rank_counts = {}\n\n    for outer in range(1, outer_iterations + 1):\n        print(f\"\\n--- Outer Iteration {outer} ---\")\n        search_keywords = []\n        while True:\n            selected_keyword = weighted_random_selection(keywords, weights)\n            if not selected_keyword:\n                logger.warning(\"No keyword selected. Ending inner iterations.\")\n                break\n            # Prevent adding duplicate keywords\n            if selected_keyword in search_keywords:\n                print(f\"Keyword '{selected_keyword}' already in query. Selecting a different keyword.\")\n                continue\n            search_keywords.append(selected_keyword)\n            query = construct_search_query(search_keywords)\n            match_count, matched_papers = execute_search_scopus(query, scopus_api_key, threshold)\n            print(f\"Added '{selected_keyword}' | Query: '{query}' | Matches: {match_count}\")\n\n            # Update progress\n            if progress_callback:\n                progress_callback(outer, query, match_count)\n\n            if match_count &lt; threshold:\n                print(f\"Match count {match_count} below threshold {threshold}. Ending inner iterations.\")\n                break\n\n            # Add a small delay to simulate processing time\n            time.sleep(0.1)\n\n        # Record matched papers from the final inner iteration\n        for paper in matched_papers:\n            scopus_id = paper['scopus_id']\n            if scopus_id in paper_rank_counts:\n                paper_rank_counts[scopus_id]['occurrences'] += 1\n            else:\n                paper['occurrences'] = 1\n                paper_rank_counts[scopus_id] = paper\n\n            #print(f\"Recorded paper: {scopus_id} | Current occurrences: {paper_rank_counts[scopus_id]['occurrences']}\")\n\n        # Add a small delay after each outer iteration\n        time.sleep(0.1)\n\n    # Create a ranked list sorted by occurrences descending\n    ranked_papers = sorted(paper_rank_counts.values(), key=lambda x: x['occurrences'], reverse=True)\n    print(\"\\n--- Sampling Completed ---\")\n    return ranked_papers\n</code></pre>"},{"location":"processing/#backend.processing.weighted_random_selection","title":"<code>weighted_random_selection(keywords, weights)</code>","text":"<p>Select a keyword based on weights.</p> <p>Parameters: - keywords: List of keywords. - weights: Corresponding list of weights.</p> <p>Returns: - Selected keyword or None if no selection possible.</p> Source code in <code>backend\\processing.py</code> <pre><code>def weighted_random_selection(keywords, weights):\n    \"\"\"\n    Select a keyword based on weights.\n\n    Parameters:\n    - keywords: List of keywords.\n    - weights: Corresponding list of weights.\n\n    Returns:\n    - Selected keyword or None if no selection possible.\n    \"\"\"\n    total_weight = sum(weights)\n    if total_weight == 0:\n        logger.info(\"Total weight is zero. No keyword can be selected.\")\n        return None\n    probabilities = [w / total_weight for w in weights]\n    selected_keyword = np.random.choice(keywords, p=probabilities)\n\n    try:\n        selected_index = keywords.index(selected_keyword)\n        weight = weights[selected_index]\n        print(f\"Selected keyword: '{selected_keyword}' with weight {weight}\", flush=True)\n    except ValueError:\n        logger.error(f\"Selected keyword '{selected_keyword}' not found in keywords list.\", flush=True)\n\n    return selected_keyword\n</code></pre>"}]}